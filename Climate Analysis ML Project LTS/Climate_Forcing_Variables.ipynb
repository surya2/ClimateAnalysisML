{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 193] %1 is not a valid Win32 application",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-54941986e379>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#from sklearn.ensemble import RandomForestRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\scipy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;31m# Allow distributors to run custom init code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0m_num\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__all__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\scipy\\_distributor_init.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibs_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibs_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'*dll'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[0mWinDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mowd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 193] %1 is not a valid Win32 application"
     ]
    }
   ],
   "source": [
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "import os\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-516fd53eaf15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhurricane_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./Datasets/Atlantic Hurricane ACE Data.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhurricane_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhurricane_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhurricane_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnumber_of_storms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhurricane_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNamed_Storms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "hurricane_data = './Datasets/Atlantic Hurricane ACE Data.csv'\n",
    "hurricane_data = pd.read_csv(hurricane_data)\n",
    "ace = hurricane_data.ACE\n",
    "number_of_storms = hurricane_data.Named_Storms\n",
    "\n",
    "hurricaneYear = hurricane_data.Year\n",
    "hurricaneTime = hurricane_data.time\n",
    "\n",
    "features = ['Year', 'time']\n",
    "hurricaneX = hurricane_data[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "import math\n",
    "#import statsmodels.api as sm\n",
    "#from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "params = {'n_estimators': 10000, 'max_depth': 4,\n",
    "          'learning_rate': 0.1, 'loss': 'ls'}\n",
    "model = GradientBoostingRegressor(**params)\n",
    "modelSVR = SVR(kernel='rbf', C=1e4, epsilon=0.1)\n",
    "#X_train = x_train = X_test = x_test = y_train = y_test = None\n",
    "\n",
    "class MachineLearningAlgorithm:\n",
    "    def __init__(self, x, X, y, n):\n",
    "        print('Additive Gradient Boosting Learning Model: ')\n",
    "        self.x = x\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.option = n\n",
    "        \n",
    "    def testSplitValid(self):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(self.x, self.y, test_size=0.2)\n",
    "        \n",
    "        '''sc_X = StandardScaler()\n",
    "        sc_y = StandardScaler()\n",
    "        X = sc_X.fit_transform(x)\n",
    "        y = sc_y.fit_transform(y)'''\n",
    "        \n",
    "        plt.scatter(x_train, y_train, color='r', s = 6, alpha=0.5)\n",
    "        plt.scatter(x_test, y_test, color='b', s = 6, alpha=0.5)\n",
    "        plt.title('Train/Test Split Graph')\n",
    "        plt.xlabel('Years')\n",
    "        plt.ylabel('Molar Ratio')\n",
    "        plt.show()\n",
    "        print(' ')\n",
    "        if self.option == 1:\n",
    "            self.gradientBoosting(X_train, x_train, y_train, X_test, x_test, y_test)\n",
    "        if self.option == 2:\n",
    "            self.supportVector(X_train, x_train, y_train, X_test, x_test, y_test)\n",
    "        if self.option == 3:\n",
    "            self.arima(X_train, x_train, y_train, X_test, x_test, y_test)\n",
    "            \n",
    "    def arima(X_train, x_train, y_train, X_test, x_test, y_test):\n",
    "        result = seasonal_decompose(self.y, model ='multiplicative')\n",
    "        print(result.plot())\n",
    "        \n",
    "                \n",
    "        \n",
    "        \n",
    "    \n",
    "    def supportVector(self, X_train, x_train, y_train, X_test, x_test, y_test):\n",
    "        print('SVR Model Training: ')\n",
    "        modelSVR.fit(X_train, y_train)\n",
    "        vectors = modelSVR.support_vectors_\n",
    "        indices = [modelSVR.support_]\n",
    "        \n",
    "        vector_value = []\n",
    "        vector_year = []\n",
    "        for i in vectors:\n",
    "            vector_value.append(i[0])\n",
    "            vector_year.append(i[1])\n",
    "            \n",
    "        train_theoretical = modelSVR.predict(X_train)\n",
    "        plt.scatter(x_train, y_train, color=\"g\", s = 6, alpha=0.5)\n",
    "        plt.scatter(x_train, train_theoretical, color='y', s = 6, alpha=0.5)\n",
    "        plt.scatter(vector_value, vector_year, color='r', s = 2, alpha=0.5)\n",
    "        plt.show()\n",
    "        print('MAE for train set: ', mean_absolute_error(y_train, train_theoretical))\n",
    "        print('RMSE for train set: ', math.sqrt(mean_squared_error(y_train, train_theoretical)))\n",
    "        print(' ')\n",
    "        \n",
    "        print(\"Validating Test Data: \")\n",
    "        test_theoretical = modelSVR.predict(X_test)\n",
    "        plt.scatter(x_test, y_test, color=\"g\", s = 6, alpha=0.5)\n",
    "        plt.scatter(x_test, test_theoretical, color='y', s = 6, alpha=0.5)\n",
    "        plt.show()\n",
    "        print('MAE for test set: ', mean_absolute_error(y_test, test_theoretical))\n",
    "        print('RMSE for test set: ', math.sqrt(mean_squared_error(y_test, test_theoretical)))\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "        \n",
    "        '''mae_list = []\n",
    "        max_leaf_node_list = [5, 50, 500, 5000]\n",
    "        for max_leaf_nodes in max_leaf_node_list:\n",
    "            my_mae = self.get_mae(max_leaf_nodes, X_train, X_test, y_train, y_test)\n",
    "            mae_list.append(my_mae)\n",
    "            print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n",
    "            print(' ')\n",
    "        \n",
    "        mae_list.sort()\n",
    "        smallest_mae = mae_list[0]\n",
    "        element_index = mae_list.index(smallest_mae)\n",
    "        final_mln = max_leaf_node_list[element_index]\n",
    "        self.finalFit(final_mln)'''\n",
    "    def gradientBoosting(self, X_train, x_train, y_train, X_test, x_test, y_test):\n",
    "        print('GB Model Training: ')\n",
    "        model.fit(X_train, y_train)\n",
    "        train_theoretical = model.predict(X_train)\n",
    "        plt.scatter(x_train, y_train, color=\"g\", s = 6, alpha=0.5)\n",
    "        plt.scatter(x_train, train_theoretical, color='y', s = 6, alpha=0.5)\n",
    "        plt.show()\n",
    "        print('MAE for train set: ', mean_absolute_error(y_train, train_theoretical))\n",
    "        print(' ')\n",
    "        \n",
    "        print(\"Validating Test Data: \")\n",
    "        test_theoretical = model.predict(X_test)\n",
    "        plt.scatter(x_test, y_test, color=\"g\", s = 6, alpha=0.5)\n",
    "        plt.scatter(x_test, test_theoretical, color='y', s = 6, alpha=0.5)\n",
    "        plt.show()\n",
    "        print('MAE for test set: ', mean_absolute_error(y_test, test_theoretical))\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "        \n",
    "        mae_list = []\n",
    "        max_leaf_node_list = [5, 50, 500, 5000]\n",
    "        for max_leaf_nodes in max_leaf_node_list:\n",
    "            my_mae = self.get_mae(max_leaf_nodes, X_train, X_test, y_train, y_test)\n",
    "            mae_list.append(my_mae)\n",
    "            print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n",
    "            print(' ')\n",
    "        \n",
    "        mae_list.sort()\n",
    "        smallest_mae = mae_list[0]\n",
    "        element_index = mae_list.index(smallest_mae)\n",
    "        final_mln = max_leaf_node_list[element_index]\n",
    "        self.finalFit(final_mln)\n",
    "        return\n",
    "        \n",
    "    def get_mae(self, mln, train_X, test_X, train_y, test_y):\n",
    "        model.fit(train_X, train_y)\n",
    "        preds = model.predict(test_X)\n",
    "        mae = mean_absolute_error(test_y, preds)\n",
    "        return(mae)\n",
    "    \n",
    "    def finalFit(self, mln):\n",
    "        self.final_model = GradientBoostingRegressor(max_leaf_nodes=mln, random_state=0)\n",
    "        self.final_model.fit(self.X, self.y)\n",
    "        theoretical_preds = self.final_model.predict(self.X)\n",
    "        print('Final Holistic Prediction: ')\n",
    "        plt.scatter(self.x, self.y, color=\"g\", s = 6, alpha=0.5)\n",
    "        plt.scatter(self.x, theoretical_preds, color='y', s = 6, alpha=0.5)\n",
    "        plt.show()\n",
    "        print('MAE for final prediction: ', mean_absolute_error(self.y, theoretical_preds))\n",
    "        return\n",
    "        \n",
    "    def predictGB(self, x, X):\n",
    "        final_preds = self.final_model.predict(X)\n",
    "        plt.scatter(self.x, self.y, color=\"g\", s = 6, alpha=0.5)\n",
    "        plt.scatter(x, final_preds, color='y', s = 6, alpha=0.5)\n",
    "        return final_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hurricaneYear, ace, color='y')\n",
    "plt.scatter(hurricaneYear, ace, color='r', s = 8, alpha=0.8)\n",
    "#plt.scatter(hurricaneYear, number_of_storms, color='g', s = 8, alpha=0.8)\n",
    "plt.title('Accumulated Cyclone Energy Index Measurements (ACE)')\n",
    "plt.xlabel('Years since 1851')\n",
    "plt.ylabel('ACE (10^4 kt^2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Smoothing and Downscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import splev, splrep\n",
    "\n",
    "x_smooth = np.linspace(hurricaneYear.min(), hurricaneYear.max(), 1000)\n",
    "x_smoothTime = np.linspace(hurricaneTime.min(), hurricaneTime.max(), 1000)\n",
    "data = {'Year': x_smooth,\n",
    "        'time': x_smoothTime\n",
    "        }\n",
    "smoothX = pd.DataFrame(data, columns = ['Year', 'time'])\n",
    "\n",
    "spl = splrep(hurricaneYear, ace)\n",
    "y_smooth = splev(x_smooth, spl)\n",
    "\n",
    "plt.plot(hurricaneYear, ace, color='y')\n",
    "plt.scatter(hurricaneYear, ace, color='r', s = 8, alpha=0.8)\n",
    "plt.plot(x_smooth, y_smooth, color='g')\n",
    "#plt.scatter(hurricaneYear, number_of_storms, color='g', s = 8, alpha=0.8)\n",
    "plt.title('Accumulated Cyclone Energy Index Measurements (ACE)')\n",
    "plt.xlabel('Years since 1851')\n",
    "plt.ylabel('ACE (10^4 kt^2)')\n",
    "plt.show()\n",
    "\n",
    "#--------\n",
    "#y_smooth = pd.DataFrame({'Y': y_smooth})\n",
    "\n",
    "def movAvg(values, window):\n",
    "    weights = np.repeat(1.0, window) /window\n",
    "    smas = np.convolve(values,weights,'valid')\n",
    "    return smas\n",
    "\n",
    "def expMovAvg(values, window):\n",
    "    weights = np.exp(np.linspace(-1.,0.,window))\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    a = np.convolve(values,weights)[:len(values)]\n",
    "    a[:window]=a[window]\n",
    "    return a\n",
    "\n",
    "y_sma = movAvg(y_smooth, 25)\n",
    "y_ema = expMovAvg(y_smooth, 25)\n",
    "data = np.array([y_sma, y_ema[:976]])\n",
    "y_mean = np.average(data, axis=0)\n",
    "\n",
    "moving_average = np.concatenate((y_sma[:24], y_ema[24:]))\n",
    "\n",
    "plt.plot(hurricaneYear, ace, color='y')\n",
    "#plt.scatter(hurricaneYear, ace, color='r', s = 8, alpha=0.8)\n",
    "plt.plot(x_smooth, y_smooth, color='g')\n",
    "plt.plot(x_smooth[:976], y_sma, color='c')\n",
    "plt.plot(x_smooth, y_ema, color='m')\n",
    "plt.plot(x_smooth[:976], y_mean, color='r')\n",
    "#plt.scatter(hurricaneYear, number_of_storms, color='g', s = 8, alpha=0.8)\n",
    "plt.title('Accumulated Cyclone Energy Index Measurements (ACE)')\n",
    "plt.xlabel('Years since 1851')\n",
    "plt.ylabel('ACE (10^4 kt^2)')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hurricaneYear, ace, color='y')\n",
    "#plt.scatter(hurricaneYear, ace, color='r', s = 8, alpha=0.8)\n",
    "plt.plot(x_smooth, y_smooth, color='g')\n",
    "plt.plot(x_smooth, moving_average, color='y')\n",
    "#plt.scatter(hurricaneYear, number_of_storms, color='g', s = 8, alpha=0.8)\n",
    "plt.title('Accumulated Cyclone Energy Index Measurements (ACE)')\n",
    "plt.xlabel('Years since 1851')\n",
    "plt.ylabel('ACE (10^4 kt^2)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hurricaneYearExtrap = list(range(1820, 2020))\n",
    "hurricaneTimeExtrap = [ x-1820 for x in hurricaneYearExtrap ]\n",
    "\n",
    "data = {'Year': hurricaneYearExtrap,\n",
    "        'time': hurricaneTimeExtrap\n",
    "        }\n",
    "hurricaneExtrapX = pd.DataFrame(data, columns = ['Year', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ace_boostingModel = MachineLearningAlgorithm(x_smooth, smoothX, y_smooth, 2)\n",
    "ace_boostingModel.testSplitValid()\n",
    "#ace_preds = ace_boostingModel.predictGB(hurricaneYearExtrap, hurricaneExtrapX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "north_tropicsModel = MachineLearningAlgorithm(SSTyear, SSTx, north_sst)\n",
    "north_tropicsModel.testSplitValid()\n",
    "north_tropics_preds = north_tropicsModel.predict(sstYearExtrap, sstExtrapX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "south_tropicsModel = MachineLearningAlgorithm(SSTyear, SSTx, south_sst)\n",
    "south_tropicsModel.testSplitValid()\n",
    "south_tropics_preds = south_tropicsModel.predict(sstYearExtrap, sstExtrapX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_tropics_anomaly = []\n",
    "optimal_tropics_SST = []\n",
    "for x in range(len(mid_tropics_preds)):\n",
    "    avg = (mid_tropics_preds[x] + north_tropics_preds[x] + south_tropics_preds[x])/3\n",
    "    optimal_tropics_anomaly.append(avg)\n",
    "    optimal_tropics_SST.append(avg+27.69221833)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrapolatedSST = pd.DataFrame({\"Year\":sstYearExtrap, \n",
    "                    \"time\":sstTimeExtrap,  \n",
    "                    \"extrapAnomaly\":optimal_tropics_anomaly,\n",
    "                    \"extrapSST\":optimal_tropics_SST}) \n",
    "\n",
    "extrapolatedSST.to_csv('./Datasets/Programmed Datasets/FinalSST.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO2merge = []  #This is used to merge all of the CO2 dataset y-values into one dataset\n",
    "CO2timeset = []  #This dataset will merge all of the years or x-values together\n",
    "\n",
    "class order:\n",
    "    def rank (self, a, b, c):\n",
    "        originalRank = [CO2time[a], GHGt[b], NOAA_GHGtime[c]]\n",
    "        rank = [CO2time[a], GHGt[b], NOAA_GHGtime[c]]\n",
    "        rank.sort()\n",
    "        least = rank[0]\n",
    "        elementId = originalRank.index(least)\n",
    "        \n",
    "        if elementId == 0:\n",
    "            CO2merge.append(CO2[a])\n",
    "            a = a+1\n",
    "        if elementId == 1:\n",
    "            CO2merge.append(CO2_1[b])\n",
    "            b = b+1\n",
    "        if elementId == 2:\n",
    "            CO2merge.append(CO2_2[c])\n",
    "            c = c+1\n",
    "            \n",
    "        return a, b, c\n",
    "        \n",
    "\n",
    "order = order()\n",
    "\n",
    "a = b = c = 0\n",
    "while a <= len(CO2year) or b <= len(GHGyear) or c <= len(NOAA_GHGyear):\n",
    "    '''order.rank(a, b, c)\n",
    "    a = a\n",
    "    b = b\n",
    "    c = c'''\n",
    "    if b == len(GHGyear):\n",
    "        GHGt[b] = 1000\n",
    "    if c == len(NOAA_GHGyear):\n",
    "        NOAA_GHGtime[c] = 1000\n",
    "        \n",
    "    if a == len(CO2year):\n",
    "        break\n",
    "    \n",
    "    originalRank = [CO2time[a], GHGt[b], NOAA_GHGtime[c]]\n",
    "    rank = [CO2time[a], GHGt[b], NOAA_GHGtime[c]]\n",
    "    rank.sort()\n",
    "    least = rank[0]\n",
    "    elementId = originalRank.index(least)\n",
    "     \n",
    "    if elementId == 0:\n",
    "        CO2merge.append(CO2[a])\n",
    "        CO2timeset.append(CO2time[a])\n",
    "        a = a+1\n",
    "    if elementId == 1:\n",
    "        CO2merge.append(CO2_1[b])\n",
    "        CO2timeset.append(GHGt[b])\n",
    "        b = b+1\n",
    "        \n",
    "    if elementId == 2:\n",
    "        CO2merge.append(CO2_2[c])\n",
    "        CO2timeset.append(NOAA_GHGtime[c])\n",
    "        c = c+1\n",
    "\n",
    "years = []\n",
    "for i in CO2timeset:\n",
    "    years.append(i+1750)\n",
    "\n",
    "data = {'Year': years,\n",
    "        'time': CO2timeset,\n",
    "        'CO2': CO2merge\n",
    "        }\n",
    "completeCO2set = pd.DataFrame(data, columns = ['Year', 'time', 'CO2'])\n",
    "completeCO2set.to_csv('./Datasets/Programmed Datasets/HolisticCO2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO2new = pd.read_csv('./Datasets/Programmed Datasets/HolisticCO2.csv')\n",
    "seasonalData = CO2new.CO2\n",
    "seasonalTime = CO2new.time\n",
    "\n",
    "seasonalData = seasonalData[42:]\n",
    "seasonalTime = seasonalTime[42:]\n",
    "seasonalT = []\n",
    "for i in seasonalTime:\n",
    "    seasonalT.append(i+1750)\n",
    "\n",
    "\n",
    "rawData = pd.read_csv('./Datasets/Programmed Datasets/HolisticCO2.csv', header=0, index_col=0)\n",
    "CO2seasonal = rawData[42:]\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "decomposition = seasonal_decompose(CO2seasonal, model='additive', freq=1)\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal \n",
    "residual = decomposition.resid\n",
    "\n",
    "trend.plot()\n",
    "seasonal.plot()\n",
    "residual.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sma(value, window):\n",
    "    weight = np.repeat(1.0, window)/window\n",
    "    sma = np.convolve(value, weight, 'valid')\n",
    "    return sma\n",
    "\n",
    "smaCO2 = sma(seasonalData, 30)\n",
    "smaTime = sma(seasonalT, 30)\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "rolmean = CO2seasonal.rolling(12).mean()\n",
    "rolstd = CO2seasonal.rolling(12).std()\n",
    "\n",
    "plt.plot(CO2seasonal['CO2'], color='green', label = 'Rolling Std')\n",
    "plt.plot(rolmean['CO2'], color='red', label='Rolling Mean')\n",
    "plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "plt.plot()\n",
    "plt.title('Carbon Dioxide Concentrations')\n",
    "plt.xlabel('Years since 1950')\n",
    "plt.ylabel('Parts per Million')\n",
    "\n",
    "#print(rolmean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
